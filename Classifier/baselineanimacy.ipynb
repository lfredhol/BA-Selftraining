{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89160468-9f07-468d-b4f9-b4fe2e698071","_uuid":"9005219e-83d6-4b8d-b2d8-193e33393c97","collapsed":false,"execution":{"iopub.execute_input":"2023-07-24T01:10:47.097354Z","iopub.status.busy":"2023-07-24T01:10:47.097010Z","iopub.status.idle":"2023-07-24T01:10:50.010647Z","shell.execute_reply":"2023-07-24T01:10:50.008196Z","shell.execute_reply.started":"2023-07-24T01:10:47.097325Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Imports\n","\n","import os\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from tqdm import tqdm\n","from sklearn.model_selection import KFold\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"993ba19a-37ef-4b4e-b348-ab0f448b289c","_uuid":"4e8ae875-1c10-421a-92e7-bccbabd37856","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T01:10:50.012045Z","iopub.status.idle":"2023-07-24T01:10:50.012841Z","shell.execute_reply":"2023-07-24T01:10:50.012617Z","shell.execute_reply.started":"2023-07-24T01:10:50.012593Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Preprocessing the data: Loading, tokenizing, and labeling the text data\n","\n","def load_data(filepath):\n","    with open(filepath, 'r') as f:\n","        lines = f.readlines()\n","\n","    tokens, labels = [], []\n","    for line in lines[5:]:\n","        line = line.strip()\n","        if not line or line.startswith(\"#Text=\"):\n","            continue\n","        parts = line.split()\n","        if len(parts) != 5:\n","            continue\n","        _, _, token, entity_label, chunk_label = parts\n","        if entity_label == \"Animated\":\n","            label = chunk_label\n","            if label == \"B\":\n","                label = \"B-Animated\"\n","            elif label == \"I\":\n","                label = \"I-Animated\"\n","        else:\n","            label = \"O\"\n","        tokens.append(token)\n","        labels.append(label)\n","\n","    return tokens, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"514b046b-54a6-4182-8d1c-71eeaf0e1efc","_uuid":"b087f913-fd4c-4fc9-99f5-eda04e0a206b","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T01:10:50.014448Z","iopub.status.idle":"2023-07-24T01:10:50.015202Z","shell.execute_reply":"2023-07-24T01:10:50.014980Z","shell.execute_reply.started":"2023-07-24T01:10:50.014957Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Set up paths and tag values, load and split data, and initialize the German BERT model for token classification\n","\n","BASE_DIR = '/kaggle/input/animacyba/Metonym/'\n","NEW_TEST_DIR = '/kaggle/input/bttestset'\n","TWITTER_TEST_DIR = '/kaggle/input/twitterset/Twitter'\n","TORE_TEST_DIR = '/kaggle/input/toredataset/'\n","\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-german-cased')\n","tag_values = [\"B-Animated\", \"I-Animated\", \"O\", \"PAD\"]\n","tag2id = {t: i for i, t in enumerate(tag_values)}\n","\n","all_files = os.listdir(BASE_DIR)\n","np.random.shuffle(all_files)\n","\n","train_files = all_files[:20000]\n","val_files = all_files[20000:22500]\n","test_files = all_files[22500:]\n","\n","model = BertForTokenClassification.from_pretrained(\n","    \"bert-base-german-cased\",\n","    num_labels=len(tag2id),\n","    output_attentions=False,\n","    output_hidden_states=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9670ffec-e5cc-464a-9204-da1d13f8df94","_uuid":"5873e4f7-8a7d-4421-a033-d6f3f2bf2b2f","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T01:10:50.016657Z","iopub.status.idle":"2023-07-24T01:10:50.017497Z","shell.execute_reply":"2023-07-24T01:10:50.017280Z","shell.execute_reply.started":"2023-07-24T01:10:50.017231Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Preprocessing the data for NER and suitable format for BERT\n","\n","class EntityDataset(Dataset):\n","    def __init__(self, texts, tags, tag2id, tokenizer):\n","        self.texts = texts\n","        self.tags = tags\n","        self.tag2id = tag2id\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        text = self.texts[item]\n","        tags = self.tags[item]\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            is_split_into_words=True,\n","            add_special_tokens=True,\n","            max_length=128,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True\n","        )\n","\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        labels = []\n","        for word, label in zip(text, tags):\n","            tokenized_word = self.tokenizer.tokenize(word)\n","            n_subwords = len(tokenized_word)\n","            labels.extend([self.tag2id.get(label, self.tag2id[\"O\"])] * n_subwords)\n","        labels = labels[:128 - 2]\n","        labels = [self.tag2id[\"O\"]] + labels + [self.tag2id[\"O\"]]\n","        labels = labels + (128 - len(labels)) * [self.tag2id[\"PAD\"]]\n","\n","        return {\n","            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n","            'labels': torch.tensor(labels, dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c3d6d49-2959-40dd-8ceb-b854c4f81aa6","_uuid":"04cfe366-0c96-493e-8515-ea7093f6938f","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T01:10:50.018876Z","iopub.status.idle":"2023-07-24T01:10:50.019688Z","shell.execute_reply":"2023-07-24T01:10:50.019453Z","shell.execute_reply.started":"2023-07-24T01:10:50.019431Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Loading and preparing the training data\n","\n","BATCH_SIZE = 32\n","\n","all_data = []\n","for file in tqdm(all_files, desc=\"Loading data\"):\n","    tokens, labels = load_data(os.path.join(BASE_DIR, file))\n","    all_data.append({\"tokens\": tokens, \"labels\": labels})\n","\n","num_train = int(len(all_data) * 0.8)\n","num_val = int(len(all_data) * 0.1)\n","num_test = len(all_data) - num_train - num_val\n","\n","train_data, val_data, test_data = random_split(all_data, [num_train, num_val, num_test])\n","\n","train_texts, train_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in train_data])\n","val_texts, val_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in val_data])\n","test_texts, test_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in test_data])\n","\n","train_dataset = EntityDataset(train_texts, train_tags, tag2id, tokenizer)\n","val_dataset = EntityDataset(val_texts, val_tags, tag2id, tokenizer)\n","test_dataset = EntityDataset(test_texts, test_tags, tag2id, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db9b463a-56eb-49b6-9d59-87091c20f510","_uuid":"0b499a8c-714f-4004-bb71-012c79fbffea","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T01:10:50.021177Z","iopub.status.idle":"2023-07-24T01:10:50.021978Z","shell.execute_reply":"2023-07-24T01:10:50.021742Z","shell.execute_reply.started":"2023-07-24T01:10:50.021721Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Training on initial training data with k-fold-cross validation\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","EPOCHS = 2\n","class_weights = torch.tensor([1, 1, 0.9, 0.1]).to(device)\n","loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","k_folds = 5\n","kfold = KFold(n_splits=k_folds, shuffle=True)\n","results = {}\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n","    print(f'Validation Fold: {fold}')\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=10, sampler=torch.utils.data.SubsetRandomSampler(train_ids))\n","    valloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=10, sampler=torch.utils.data.SubsetRandomSampler(test_ids))\n","\n","    model.train()\n","    total_loss = 0\n","\n","    # Training loop\n","    for _, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n","        inputs = {\n","            \"input_ids\": data['input_ids'].to(device), \n","            \"attention_mask\": data['attention_mask'].to(device), \n","            \"labels\": data['labels'].to(device)\n","        }\n","       \n","        optimizer.zero_grad()\n","        outputs = model(**inputs)\n","        loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), inputs[\"labels\"].view(-1))\n","        loss.backward()\n","       \n","        total_loss += loss.item()\n","        optimizer.step()\n","       \n","    print(\"Average train loss: {}\".format(total_loss / len(trainloader)))\n","\n","    model.eval()\n","    eval_loss = 0\n","\n","    for _, data in enumerate(valloader, 0):\n","        inputs = {\n","            \"input_ids\": data['input_ids'].to(device), \n","            \"attention_mask\": data['attention_mask'].to(device), \n","            \"labels\": data['labels'].to(device)\n","        }\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            eval_loss += outputs[0].item()  # [0] because we just need the loss from the outputs\n","           \n","    print(\"Validation loss: {}\".format(eval_loss / len(valloader)))\n","    results[fold] = eval_loss / len(valloader)\n","\n","print(f'Results for: {k_folds} folds')\n","print(f'Average: {sum(results.values())/k_folds}')\n","\n","for key, value in results.items():\n","    print(f'Fold {key}: {value}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57536c06-327a-4a27-8cdb-ffa754ac8ca0","_uuid":"950db753-964a-4dd9-a473-1c5b572ec0d7","collapsed":false,"execution":{"iopub.status.busy":"2023-07-24T01:10:50.023436Z","iopub.status.idle":"2023-07-24T01:10:50.024182Z","shell.execute_reply":"2023-07-24T01:10:50.023969Z","shell.execute_reply.started":"2023-07-24T01:10:50.023947Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluation on the test set from the training data and creation of confusion matrix\n","\n","model.eval()\n","\n","predictions, true_labels = [], []\n","\n","for batch in test_loader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","    \n","    with torch.no_grad():\n","        outputs = model(**batch)\n","        \n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","\n","    predictions.extend(np.argmax(logits, axis=2))\n","    true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","predictions_no_pad = [pred for pred, true in zip(np.hstack(predictions), np.hstack(true_labels)) if true != PAD_TOKEN_ID]\n","true_labels_no_pad = [true for true in np.hstack(true_labels) if true != PAD_TOKEN_ID]\n","\n","accuracy = accuracy_score(true_labels_no_pad, predictions_no_pad)\n","precision, recall, f1, _ = precision_recall_fscore_support(true_labels_no_pad, predictions_no_pad, average='weighted')\n","\n","def recode_labels(label_list):\n","    return [0 if label == tag2id[\"O\"] else 1 for label in label_list]\n","\n","recoded_predictions = recode_labels(predictions_no_pad)\n","recoded_true_labels = recode_labels(true_labels_no_pad)\n","recoded_cm = confusion_matrix(recoded_true_labels, recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(test_loader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in predictions[i]])}\")\n","    print(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:10:50.025610Z","iopub.status.idle":"2023-07-24T01:10:50.026455Z","shell.execute_reply":"2023-07-24T01:10:50.026208Z","shell.execute_reply.started":"2023-07-24T01:10:50.026186Z"},"trusted":true},"outputs":[],"source":["# Evaluation on the seen test set of german bundestag debates and creation of confusion matrix\n","\n","model = model.to(device)\n","\n","test_files = os.listdir(NEW_TEST_DIR)\n","test_data = []\n","for file in tqdm(test_files, desc=\"Loading test data\"):\n","    tokens, labels = load_data(os.path.join(NEW_TEST_DIR, file))\n","    test_data.append({\"tokens\": tokens, \"labels\": labels})\n","\n","test_texts, test_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in test_data])\n","test_dataset = EntityDataset(test_texts, test_tags, tag2id, tokenizer)\n","testbt_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","\n","model.eval()\n","bt_predictions, bt_true_labels = [], []\n","\n","for batch in testbt_loader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","    \n","    bt_predictions.extend(np.argmax(logits, axis=2))\n","    bt_true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","bt_predictions_no_pad = [pred for pred, true in zip(np.hstack(bt_predictions), np.hstack(bt_true_labels)) if true != PAD_TOKEN_ID]\n","bt_true_labels_no_pad = [true for true in np.hstack(bt_true_labels) if true != PAD_TOKEN_ID]\n","\n","bt_accuracy = accuracy_score(bt_true_labels_no_pad, bt_predictions_no_pad)\n","bt_precision, bt_recall, bt_f1, _ = precision_recall_fscore_support(bt_true_labels_no_pad, bt_predictions_no_pad, average='weighted', zero_division=1)\n","\n","bt_recoded_predictions = recode_labels(bt_predictions_no_pad)\n","bt_recoded_true_labels = recode_labels(bt_true_labels_no_pad)\n","bt_recoded_cm = confusion_matrix(bt_recoded_true_labels, bt_recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(bt_recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(testbt_loader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in bt_true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in bt_predictions[i]])}\")\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:10:50.027840Z","iopub.status.idle":"2023-07-24T01:10:50.028597Z","shell.execute_reply":"2023-07-24T01:10:50.028381Z","shell.execute_reply.started":"2023-07-24T01:10:50.028359Z"},"trusted":true},"outputs":[],"source":["# Evaluation on the unseen test set of Tweets and creation of confusion matrix\n","\n","model = model.to(device)\n","test_files_path = os.listdir(TWITTER_TEST_DIR)\n","tw_test_data = []\n","\n","for file in tqdm(test_files_path, desc=\"Loading test data\"):\n","    file_tokens, file_labels = load_data(os.path.join(TWITTER_TEST_DIR, file))\n","    tw_test_data.append({\"tokens\": file_tokens, \"labels\": file_labels})\n","\n","tw_test_text, tw_test_tags = zip(*[(data[\"tokens\"], data[\"labels\"]) for data in tw_test_data])\n","tw_test_dataset = EntityDataset(tw_test_text, tw_test_tags, tag2id, tokenizer)\n","tw_test_dataloader = DataLoader(tw_test_dataset, batch_size=BATCH_SIZE)\n","\n","model.eval()\n","\n","tw_predictions, tw_true_labels = [], []\n","\n","for batch in tw_test_dataloader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","\n","    tw_predictions.extend(np.argmax(logits, axis=2))\n","    tw_true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","tw_predictions_no_pad = [pred for pred, true in zip(np.hstack(tw_predictions), np.hstack(tw_true_labels)) if true != PAD_TOKEN_ID]\n","tw_true_labels_no_pad = [true for true in np.hstack(tw_true_labels) if true != PAD_TOKEN_ID]\n","\n","tw_accuracy = accuracy_score(tw_true_labels_no_pad, tw_predictions_no_pad)\n","tw_precision, tw_recall, tw_f1, _ = precision_recall_fscore_support(tw_true_labels_no_pad, tw_predictions_no_pad, average='weighted', zero_division=1)\n","\n","tw_recoded_predictions = recode_labels(tw_predictions_no_pad)\n","tw_recoded_true_labels = recode_labels(tw_true_labels_no_pad)\n","tw_recoded_cm = confusion_matrix(tw_recoded_true_labels, tw_recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(tw_recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(tw_test_dataloader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in tw_true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in tw_predictions[i]])}\")\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-24T01:10:50.030045Z","iopub.status.idle":"2023-07-24T01:10:50.030857Z","shell.execute_reply":"2023-07-24T01:10:50.030635Z","shell.execute_reply.started":"2023-07-24T01:10:50.030610Z"},"trusted":true},"outputs":[],"source":["# Evaluation on the unseen test set of german bundestag debates and creation of confusion matrix\n","\n","model = model.to(device)\n","test_files_path = os.listdir(NEW_TEST_DIR)\n","test_data = []\n","\n","for file in tqdm(test_files_path, desc=\"Loading test data\"):\n","    file_tokens, file_labels = load_data(os.path.join(TEST_DIR, file))\n","    test_data.append({\"tokens\": file_tokens, \"labels\": file_labels})\n","\n","# Prepare test data for DataLoader\n","test_text, test_tags = zip(*[(data[\"tokens\"], data[\"labels\"]) for data in test_data])\n","test_dataset = EntityDataset(test_text, test_tags, tag2id, tokenizer)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","\n","model.eval()\n","predictions, true_labels = [], []\n","\n","for batch in test_dataloader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","\n","    predictions.extend(np.argmax(logits, axis=2))\n","    true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","predictions_no_pad = [pred for pred, true in zip(np.hstack(predictions), np.hstack(true_labels)) if true != PAD_TOKEN_ID]\n","true_labels_no_pad = [true for true in np.hstack(true_labels) if true != PAD_TOKEN_ID]\n","\n","accuracy = accuracy_score(true_labels_no_pad, predictions_no_pad)\n","precision, recall, f1, _ = precision_recall_fscore_support(true_labels_no_pad, predictions_no_pad, average='weighted', zero_division=1)\n","\n","recoded_predictions = recode_labels(predictions_no_pad)\n","recoded_true_labels = recode_labels(true_labels_no_pad)\n","recoded_cm = confusion_matrix(recoded_true_labels, recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(test_dataloader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in predictions[i]])}\")\n","    print(\"\\n\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
