{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a73bd7fa-037a-4f57-8921-031e1c29003c","_uuid":"4f59130d-2d3b-4c59-92c1-0241d0b2b755","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Imports\n","\n","import os\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from tqdm import tqdm\n","from sklearn.model_selection import KFold\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37cb488a-e5c9-4337-9f32-a94351f207ad","_uuid":"7b820927-9e36-47af-b458-da5d22e9cee9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Preprocessing the data: Loading, tokenizing, and labeling the text data\n","\n","def load_data(filepath):\n","    with open(filepath, 'r') as f:\n","        lines = f.readlines()\n","\n","    tokens, labels = [], []\n","    for line in lines[5:]:\n","        line = line.strip()\n","        if not line or line.startswith(\"#Text=\"):\n","            continue\n","        parts = line.split()\n","        if len(parts) != 5:\n","            continue\n","        _, _, token, entity_label, chunk_label = parts\n","        if entity_label == \"Animated\":\n","            label = chunk_label\n","            if label == \"B\":\n","                label = \"B-Animated\"\n","            elif label == \"I\":\n","                label = \"I-Animated\"\n","        else:\n","            label = \"O\"\n","        tokens.append(token)\n","        labels.append(label)\n","\n","    return tokens, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8948d69-095f-4a31-8f41-4512040dea79","_uuid":"a7039b73-ab86-4204-ad7f-15d25b1ba257","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Set up paths and tag values, load and split data, and initialize the German BERT model for token classification\n","\n","BASE_DIR = '/kaggle/input/animacyba/Metonym/'\n","NEW_TEST_DIR = '/kaggle/input/bttestset'\n","TWITTER_TEST_DIR = '/kaggle/input/twitterset/Twitter'\n","TORE_TEST_DIR = '/kaggle/input/toredataset/'\n","\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-german-cased')\n","tag_values = [\"B-Animated\", \"I-Animated\", \"O\", \"PAD\"]\n","tag2id = {t: i for i, t in enumerate(tag_values)}\n","\n","all_files = os.listdir(BASE_DIR)\n","np.random.shuffle(all_files)\n","\n","train_files = all_files[:20000]\n","val_files = all_files[20000:22500]\n","test_files = all_files[22500:]\n","\n","model = BertForTokenClassification.from_pretrained(\n","    \"bert-base-german-cased\",\n","    num_labels=len(tag2id),\n","    output_attentions=False,\n","    output_hidden_states=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6752deb-7d9a-4a7f-b0d5-697953b65676","_uuid":"24cc9b0e-4160-4aa9-90d2-b06be446c801","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Preprocessing the data for NER and suitable format for BERT\n","\n","class EntityDataset(Dataset):\n","    def __init__(self, texts, tags, tag2id, tokenizer):\n","        self.texts = texts\n","        self.tags = tags\n","        self.tag2id = tag2id\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        text = self.texts[item]\n","        tags = self.tags[item]\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            is_split_into_words=True,\n","            add_special_tokens=True,\n","            max_length=128,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True\n","        )\n","\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        labels = []\n","        for word, label in zip(text, tags):\n","            tokenized_word = self.tokenizer.tokenize(word)\n","            n_subwords = len(tokenized_word)\n","            labels.extend([self.tag2id.get(label, self.tag2id[\"O\"])] * n_subwords)\n","        labels = labels[:128 - 2]\n","        labels = [self.tag2id[\"O\"]] + labels + [self.tag2id[\"O\"]]\n","        labels = labels + (128 - len(labels)) * [self.tag2id[\"PAD\"]]\n","\n","        return {\n","            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n","            'labels': torch.tensor(labels, dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3adffccb-037f-4aaf-a211-43c7ee972004","_uuid":"7432b866-8577-4ee7-ae31-313346f3d541","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Loading and preparing the training data\n","\n","BATCH_SIZE = 32\n","\n","all_data = []\n","for file in tqdm(all_files, desc=\"Loading data\"):\n","    tokens, labels = load_data(os.path.join(BASE_DIR, file))\n","    all_data.append({\"tokens\": tokens, \"labels\": labels})\n","\n","num_train = int(len(all_data) * 0.8)\n","num_val = int(len(all_data) * 0.1)\n","num_test = len(all_data) - num_train - num_val\n","\n","train_data, val_data, test_data = random_split(all_data, [num_train, num_val, num_test])\n","\n","train_texts, train_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in train_data])\n","val_texts, val_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in val_data])\n","test_texts, test_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in test_data])\n","\n","train_dataset = EntityDataset(train_texts, train_tags, tag2id, tokenizer)\n","val_dataset = EntityDataset(val_texts, val_tags, tag2id, tokenizer)\n","test_dataset = EntityDataset(test_texts, test_tags, tag2id, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2dd5707-94a2-4e24-8f5b-db460d9511e1","_uuid":"13365fac-de6c-4e63-90da-c234ae742930","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Training on initial training data with k-fold-cross validation\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","EPOCHS = 2\n","class_weights = torch.tensor([1, 1, 0.9, 0.1]).to(device)\n","loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","k_folds = 5\n","kfold = KFold(n_splits=k_folds, shuffle=True)\n","results = {}\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.001)\n","\n","for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n","    print(f'Validation Fold: {fold}')\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=10, sampler=torch.utils.data.SubsetRandomSampler(train_ids))\n","    valloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=10, sampler=torch.utils.data.SubsetRandomSampler(test_ids))\n","\n","    model.train()\n","    total_loss = 0\n","\n","    for _, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n","        inputs = {\n","            \"input_ids\": data['input_ids'].to(device), \n","            \"attention_mask\": data['attention_mask'].to(device), \n","            \"labels\": data['labels'].to(device)\n","        }\n","       \n","        optimizer.zero_grad()\n","        outputs = model(**inputs)\n","        loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), inputs[\"labels\"].view(-1))\n","        loss.backward()\n","       \n","        total_loss += loss.item()\n","        optimizer.step()\n","       \n","    print(\"Average train loss: {}\".format(total_loss / len(trainloader)))\n","\n","    model.eval()\n","    eval_loss = 0\n","\n","    for _, data in enumerate(valloader, 0):\n","        inputs = {\n","            \"input_ids\": data['input_ids'].to(device), \n","            \"attention_mask\": data['attention_mask'].to(device), \n","            \"labels\": data['labels'].to(device)\n","        }\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            eval_loss += outputs[0].item() \n","           \n","    print(\"Validation loss: {}\".format(eval_loss / len(valloader)))\n","    results[fold] = eval_loss / len(valloader)\n","\n","print(f'Results for: {k_folds} folds')\n","print(f'Average: {sum(results.values())/k_folds}')\n","\n","for key, value in results.items():\n","    print(f'Fold {key}: {value}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef28e199-ad4e-4155-87b2-938d32806af6","_uuid":"72caf57b-6ad6-426f-ba19-7d584d3033c3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Preprocessing the data: Loading, tokenizing, and labeling the selftraining data\n","\n","BASE_DIR_SELFTRAINING = '/kaggle/input/selftraining/Selftraining/'\n","\n","all_data_selftraining = []\n","for file in tqdm(os.listdir(BASE_DIR_SELFTRAINING), desc=\"Loading selftraining data\"):\n","    tokens, labels = load_data(os.path.join(BASE_DIR_SELFTRAINING, file))\n","    all_data_selftraining.append({\"tokens\": tokens, \"labels\": labels})"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e95dfcf-6409-42a8-8730-7404dd10c806","_uuid":"9c857e86-41b2-4cba-b372-8ef01c8f074a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Training on self-training data using self-training technique\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","selftrain_epochs = 1\n","confidence_threshold = 1.0\n","\n","model.eval()\n","for cycle in range(5):\n","    selected_files = np.random.choice(os.listdir(BASE_DIR_SELFTRAINING), 500)\n","\n","    all_data_selftraining = []\n","    for file in selected_files:\n","        tokens, labels = load_data(os.path.join(BASE_DIR_SELFTRAINING, file))\n","        all_data_selftraining.append({\"tokens\": tokens, \"labels\": labels})\n","        \n","    all_data_selftraining.sort(key=lambda x: len(x['tokens']))\n","\n","    selftrain_texts, _ = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in all_data_selftraining])\n","\n","    dummy_tags = [[\"O\"]*len(text) for text in selftrain_texts]\n","    selftrain_dataset = EntityDataset(selftrain_texts, dummy_tags, tag2id, tokenizer)\n","    selftrain_loader = DataLoader(selftrain_dataset, batch_size=BATCH_SIZE)\n","\n","    selftrain_predictions = []\n","    selftrain_confidences = []\n","\n","    model.eval()\n","    for batch in selftrain_loader:\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_input_mask = batch['attention_mask'].to(device)\n","      \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","        logits = outputs.logits.detach().cpu().numpy()\n","        probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n","\n","        selftrain_predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","        selftrain_confidences.extend([list(p) for p in np.max(probabilities, axis=2)])\n","\n","    selftrain_predicted_tags = [[id2tag[id] for id in sent] for sent in selftrain_predictions]\n","    selftrain_confident = [[conf > confidence_threshold for conf in confidences] for confidences in selftrain_confidences]\n","\n","    for i in range(len(all_data_selftraining)):\n","        confident_predicted_tags = [tag if confident else 'O' for tag, confident in zip(selftrain_predicted_tags[i], selftrain_confident[i])]\n","        all_data_selftraining[i][\"labels\"] = confident_predicted_tags\n","        \n","    train_data = list(train_data)\n","    train_data.extend(all_data_selftraining)\n","    train_texts, train_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in train_data])\n","    train_dataset = EntityDataset(train_texts, train_tags, tag2id, tokenizer)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","\n","\n","    for epoch in range(selftrain_epochs):\n","        model.train()\n","        total_loss = 0\n","        for _, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n","            inputs = {\"input_ids\": data['input_ids'].to(device), \"attention_mask\": data['attention_mask'].to(device), \"labels\": data['labels'].to(device)}\n","            \n","            optimizer.zero_grad() \n","            outputs = model(**inputs)\n","            loss = outputs[0]\n","            loss.backward()\n","            \n","            total_loss += loss.item()\n","            \n","            optimizer.step()\n","        \n","        avg_train_loss = total_loss / len(train_loader)\n","        print(\"Average train loss: {}\".format(avg_train_loss))\n","        \n","        model.eval()\n","        eval_loss, eval_accuracy = 0, 0\n","        for _, data in enumerate(val_loader, 0):\n","            inputs = {\"input_ids\": data['input_ids'].to(device), \"attention_mask\": data['attention_mask'].to(device), \"labels\": data['labels'].to(device)}\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","                tmp_eval_loss, logits = outputs[:2]\n","                eval_loss += tmp_eval_loss.item()\n","                \n","        eval_loss = eval_loss / len(val_loader)\n","        print(\"Validation loss: {}\".format(eval_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcee9691-8cc8-46af-acea-c171091f68ea","_uuid":"c240c648-9786-4c40-948d-5e6fa483954e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluation on the test set from the training data and creation of confusion matrix\n","\n","model.eval()\n","\n","predictions, true_labels = [], []\n","\n","for batch in test_loader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","    \n","    with torch.no_grad():\n","        outputs = model(**batch)\n","        \n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","\n","    predictions.extend(np.argmax(logits, axis=2))\n","    true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","predictions_no_pad = [pred for pred, true in zip(np.hstack(predictions), np.hstack(true_labels)) if true != PAD_TOKEN_ID]\n","true_labels_no_pad = [true for true in np.hstack(true_labels) if true != PAD_TOKEN_ID]\n","\n","accuracy = accuracy_score(true_labels_no_pad, predictions_no_pad)\n","precision, recall, f1, _ = precision_recall_fscore_support(true_labels_no_pad, predictions_no_pad, average='weighted')\n","\n","def recode_labels(label_list):\n","    return [0 if label == tag2id[\"O\"] else 1 for label in label_list]\n","\n","recoded_predictions = recode_labels(predictions_no_pad)\n","recoded_true_labels = recode_labels(true_labels_no_pad)\n","recoded_cm = confusion_matrix(recoded_true_labels, recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(test_loader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in predictions[i]])}\")\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5957032-ffcd-460d-a081-3762abc024dd","_uuid":"415c272b-e22d-49e6-b181-fc16531f0343","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluation on the seen test set of german bundestag debates and creation of confusion matrix\n","\n","model = model.to(device)\n","\n","test_files = os.listdir(NEW_TEST_DIR)\n","test_data = []\n","for file in tqdm(test_files, desc=\"Loading test data\"):\n","    tokens, labels = load_data(os.path.join(NEW_TEST_DIR, file))\n","    test_data.append({\"tokens\": tokens, \"labels\": labels})\n","\n","test_texts, test_tags = zip(*[(data_dict[\"tokens\"], data_dict[\"labels\"]) for data_dict in test_data])\n","test_dataset = EntityDataset(test_texts, test_tags, tag2id, tokenizer)\n","testbt_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","\n","model.eval()\n","bt_predictions, bt_true_labels = [], []\n","\n","for batch in testbt_loader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","    \n","    bt_predictions.extend(np.argmax(logits, axis=2))\n","    bt_true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","bt_predictions_no_pad = [pred for pred, true in zip(np.hstack(bt_predictions), np.hstack(bt_true_labels)) if true != PAD_TOKEN_ID]\n","bt_true_labels_no_pad = [true for true in np.hstack(bt_true_labels) if true != PAD_TOKEN_ID]\n","\n","bt_accuracy = accuracy_score(bt_true_labels_no_pad, bt_predictions_no_pad)\n","bt_precision, bt_recall, bt_f1, _ = precision_recall_fscore_support(bt_true_labels_no_pad, bt_predictions_no_pad, average='weighted', zero_division=1)\n","\n","bt_recoded_predictions = recode_labels(bt_predictions_no_pad)\n","bt_recoded_true_labels = recode_labels(bt_true_labels_no_pad)\n","bt_recoded_cm = confusion_matrix(bt_recoded_true_labels, bt_recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(bt_recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(testbt_loader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in bt_true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in bt_predictions[i]])}\")\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"518ee779-921a-4306-a384-a2872e795e76","_uuid":"d717c3e3-09dd-4456-b9d3-3a36a499e1db","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluation on the unseen test set of Tweets and creation of confusion matrix\n","\n","model = model.to(device)\n","test_files_path = os.listdir(TWITTER_TEST_DIR)\n","tw_test_data = []\n","\n","for file in tqdm(test_files_path, desc=\"Loading test data\"):\n","    file_tokens, file_labels = load_data(os.path.join(TWITTER_TEST_DIR, file))\n","    tw_test_data.append({\"tokens\": file_tokens, \"labels\": file_labels})\n","\n","tw_test_text, tw_test_tags = zip(*[(data[\"tokens\"], data[\"labels\"]) for data in tw_test_data])\n","tw_test_dataset = EntityDataset(tw_test_text, tw_test_tags, tag2id, tokenizer)\n","tw_test_dataloader = DataLoader(tw_test_dataset, batch_size=BATCH_SIZE)\n","\n","model.eval()\n","\n","tw_predictions, tw_true_labels = [], []\n","\n","for batch in tw_test_dataloader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","\n","    tw_predictions.extend(np.argmax(logits, axis=2))\n","    tw_true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","tw_predictions_no_pad = [pred for pred, true in zip(np.hstack(tw_predictions), np.hstack(tw_true_labels)) if true != PAD_TOKEN_ID]\n","tw_true_labels_no_pad = [true for true in np.hstack(tw_true_labels) if true != PAD_TOKEN_ID]\n","\n","tw_accuracy = accuracy_score(tw_true_labels_no_pad, tw_predictions_no_pad)\n","tw_precision, tw_recall, tw_f1, _ = precision_recall_fscore_support(tw_true_labels_no_pad, tw_predictions_no_pad, average='weighted', zero_division=1)\n","\n","tw_recoded_predictions = recode_labels(tw_predictions_no_pad)\n","tw_recoded_true_labels = recode_labels(tw_true_labels_no_pad)\n","tw_recoded_cm = confusion_matrix(tw_recoded_true_labels, tw_recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(tw_recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(tw_test_dataloader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in tw_true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in tw_predictions[i]])}\")\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3952b75-d967-441a-8f12-774cd3c68845","_uuid":"feea527a-90ee-40e9-90c1-056e8ac4c3f2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Evaluation on the unseen test set of german bundestag debates and creation of confusion matrix\n","\n","model = model.to(device)\n","test_files_path = os.listdir(NEW_TEST_DIR)\n","test_data = []\n","\n","for file in tqdm(test_files_path, desc=\"Loading test data\"):\n","    file_tokens, file_labels = load_data(os.path.join(TEST_DIR, file))\n","    test_data.append({\"tokens\": file_tokens, \"labels\": file_labels})\n","\n","test_text, test_tags = zip(*[(data[\"tokens\"], data[\"labels\"]) for data in test_data])\n","test_dataset = EntityDataset(test_text, test_tags, tag2id, tokenizer)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","\n","model.eval()\n","predictions, true_labels = [], []\n","\n","for batch in test_dataloader:\n","    batch = {key: val.to(device) for key, val in batch.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits.detach().cpu().numpy()\n","    label_ids = batch['labels'].to('cpu').numpy()\n","\n","    predictions.extend(np.argmax(logits, axis=2))\n","    true_labels.extend(label_ids)\n","\n","PAD_TOKEN_ID = tag2id[\"PAD\"]\n","predictions_no_pad = [pred for pred, true in zip(np.hstack(predictions), np.hstack(true_labels)) if true != PAD_TOKEN_ID]\n","true_labels_no_pad = [true for true in np.hstack(true_labels) if true != PAD_TOKEN_ID]\n","\n","accuracy = accuracy_score(true_labels_no_pad, predictions_no_pad)\n","precision, recall, f1, _ = precision_recall_fscore_support(true_labels_no_pad, predictions_no_pad, average='weighted', zero_division=1)\n","\n","\n","recoded_predictions = recode_labels(predictions_no_pad)\n","recoded_true_labels = recode_labels(true_labels_no_pad)\n","recoded_cm = confusion_matrix(recoded_true_labels, recoded_predictions)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(recoded_cm, annot=True, fmt='d')\n","plt.title('Recoded Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","id2tag = {i: t for t, i in tag2id.items()}\n","num_samples_to_display = 10\n","for i in range(num_samples_to_display):\n","    print(f\"Text: {tokenizer.decode(test_dataloader.dataset[i]['input_ids'])}\")\n","    print(f\"True labels: {' '.join([id2tag[id] for id in true_labels[i]])}\")\n","    print(f\"Predicted labels: {' '.join([id2tag[id] for id in predictions[i]])}\")\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"956a9255-b532-4ebb-a6ee-391860881a71","_uuid":"fd6ecfff-f314-4490-87df-c7c96e12f49e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
